{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "import csv\n",
    "from pylab import *\n",
    "from matplotlib import gridspec\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "cdict = {'red': ((0.0, 1.0, 1.0),\n",
    "                 (0.125, 1.0, 1.0),\n",
    "                 (0.25, 1.0, 1.0),\n",
    "                 (0.5625, 1.0, 1.0),\n",
    "                 (1.0, 0.0, 0.0)),\n",
    "         'green': ((0.0, 0.0, 0.0),\n",
    "                   (0.25, 0.0, 0.0),\n",
    "                   (0.5625, 1.0, 1.0),\n",
    "                   (1.0, 1.0, 1.0)),\n",
    "         'blue': ((0.0, 0.0, 0.0),\n",
    "                  (0.5, 0.0, 0.0),\n",
    "                  (1.0, 0.0, 0.0))}\n",
    "my_cmap = matplotlib.colors.LinearSegmentedColormap('my_colormap',cdict,256)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73014, 10, 5, 9)\n",
      "(73014,)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "(samples,labels) = pickle.load( open( \"data_new.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "print(samples.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11088, 10, 5, 9)\n",
      "(11088, 2)\n",
      "(2376, 10, 5, 9)\n",
      "(2376, 2)\n",
      "(2376, 10, 5, 9)\n",
      "(2376, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "def class_balance(samples,labels):\n",
    "    samples0 = samples[labels==0,:,:,:]\n",
    "    samples1 = samples[labels==1,:,:,:]\n",
    "    selected = np.random.choice(len(samples0), len(samples1), replace=False)\n",
    "    samples0 = samples0[selected,:,:,:]\n",
    "    labels0 = np.zeros((len(samples0)))\n",
    "    labels1 = np.ones((len(samples1)))\n",
    "    samples = np.concatenate((samples0,samples1),axis=0)\n",
    "    labels = np.concatenate((labels0,labels1),axis=0)\n",
    "    samples, labels = shuffle(samples, labels)\n",
    "    return samples, labels\n",
    "def one_hot(labels):\n",
    "    one_hot_labels = np.zeros((labels.shape[0],2))\n",
    "    one_hot_labels[labels==0,0]=1\n",
    "    one_hot_labels[labels==1,1]=1\n",
    "    return one_hot_labels\n",
    "\n",
    "\n",
    "samples, labels = class_balance(samples,labels)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "samples, samples_rest, labels, labels_rest = \\\n",
    "train_test_split(samples, labels, test_size=0.3, random_state=0)\n",
    "samples_validation, samples_test, labels_validation, labels_test = \\\n",
    "train_test_split(samples_rest, labels_rest, test_size=0.5, random_state=0)\n",
    "\n",
    "one_hot_labels = one_hot(labels)\n",
    "one_hot_labels_validation = one_hot(labels_validation)\n",
    "one_hot_labels_test = one_hot(labels_test)\n",
    "\n",
    "print(samples.shape)\n",
    "print(one_hot_labels.shape)\n",
    "print(samples_validation.shape)\n",
    "print(one_hot_labels_validation.shape)\n",
    "print(samples_test.shape)\n",
    "print(one_hot_labels_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MinMax_Normalization(samples):\n",
    "    samples_shape = samples.shape\n",
    "    samples = np.reshape(samples,(samples_shape[0],samples_shape[1]*samples_shape[2]*samples_shape[3]))\n",
    "    scaler = MinMaxScaler().fit(samples)\n",
    "    samples_normalized = scaler.transform(samples)\n",
    "    samples_normalized = np.reshape(samples_normalized,(samples_shape[0],samples_shape[1],samples_shape[2],samples_shape[3]))\n",
    "    return samples_normalized, scaler\n",
    "\n",
    "def transfer_scale(samples,scaler):\n",
    "    samples_shape = samples.shape\n",
    "    samples = np.reshape(samples,(samples_shape[0],samples_shape[1]*samples_shape[2]*samples_shape[3]))\n",
    "    samples_normalized = scaler.transform(samples)\n",
    "    samples_normalized = np.reshape(samples_normalized,(samples_shape[0],samples_shape[1],samples_shape[2],samples_shape[3]))\n",
    "    return samples_normalized\n",
    "    \n",
    "samples_scaled, scaler = MinMax_Normalization(samples)\n",
    "samples_scaled_validation = transfer_scale(samples_validation,scaler)\n",
    "samples_scaled_test = transfer_scale(samples_test,scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11088, 10, 5, 5)\n",
      "(2376, 10, 5, 5)\n",
      "(2376, 10, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "def feature_select(samples_scaled,features):\n",
    "    return samples_scaled[:,:,:,features]\n",
    "\n",
    "features = [0,1,2,7,8]\n",
    "samples_scaled = feature_select(samples_scaled,features)\n",
    "samples_scaled_validation = feature_select(samples_scaled_validation,features)\n",
    "samples_scaled_test = feature_select(samples_scaled_test,features)\n",
    "print(samples_scaled.shape)\n",
    "print(samples_scaled_validation.shape)\n",
    "print(samples_scaled_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Reshape, Dense, Convolution2D, Deconvolution2D, Flatten, Input, Dropout, MaxPooling2D, Activation\n",
    "from keras.models import model_from_json\n",
    "from keras.activations import relu, softmax, linear\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11088 samples, validate on 2376 samples\n",
      "Epoch 1/100\n",
      "11088/11088 [==============================] - 1s - loss: 0.6198 - acc: 0.6540 - val_loss: 0.5875 - val_acc: 0.7088\n",
      "Epoch 2/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5850 - acc: 0.6903 - val_loss: 0.5881 - val_acc: 0.6827\n",
      "Epoch 3/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5764 - acc: 0.6953 - val_loss: 0.5749 - val_acc: 0.6970\n",
      "Epoch 4/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5716 - acc: 0.6953 - val_loss: 0.6075 - val_acc: 0.6684\n",
      "Epoch 5/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5703 - acc: 0.6985 - val_loss: 0.5669 - val_acc: 0.7247\n",
      "Epoch 6/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5669 - acc: 0.7033 - val_loss: 0.5673 - val_acc: 0.7045\n",
      "Epoch 7/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5657 - acc: 0.7049 - val_loss: 0.5637 - val_acc: 0.6978\n",
      "Epoch 8/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5669 - acc: 0.7001 - val_loss: 0.5552 - val_acc: 0.7109\n",
      "Epoch 9/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5604 - acc: 0.7088 - val_loss: 0.5622 - val_acc: 0.7163\n",
      "Epoch 10/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5610 - acc: 0.7086 - val_loss: 0.5569 - val_acc: 0.7168\n",
      "Epoch 11/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5545 - acc: 0.7131 - val_loss: 0.5546 - val_acc: 0.7294\n",
      "Epoch 12/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5519 - acc: 0.7135 - val_loss: 0.5607 - val_acc: 0.7016\n",
      "Epoch 13/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5522 - acc: 0.7126 - val_loss: 0.5602 - val_acc: 0.7138\n",
      "Epoch 14/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5507 - acc: 0.7133 - val_loss: 0.5485 - val_acc: 0.7239\n",
      "Epoch 15/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5456 - acc: 0.7220 - val_loss: 0.5439 - val_acc: 0.7294\n",
      "Epoch 16/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5431 - acc: 0.7173 - val_loss: 0.5489 - val_acc: 0.7247\n",
      "Epoch 17/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5438 - acc: 0.7175 - val_loss: 0.5482 - val_acc: 0.7256\n",
      "Epoch 18/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5443 - acc: 0.7158 - val_loss: 0.5496 - val_acc: 0.7319\n",
      "Epoch 19/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5393 - acc: 0.7238 - val_loss: 0.5389 - val_acc: 0.7353\n",
      "Epoch 20/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5359 - acc: 0.7243 - val_loss: 0.5519 - val_acc: 0.7260\n",
      "Epoch 21/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5371 - acc: 0.7229 - val_loss: 0.5466 - val_acc: 0.7264\n",
      "Epoch 22/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5341 - acc: 0.7229 - val_loss: 0.5550 - val_acc: 0.7155\n",
      "Epoch 23/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5316 - acc: 0.7256 - val_loss: 0.5401 - val_acc: 0.7306\n",
      "Epoch 24/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5338 - acc: 0.7236 - val_loss: 0.5451 - val_acc: 0.7319\n",
      "Epoch 25/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5287 - acc: 0.7315 - val_loss: 0.5347 - val_acc: 0.7395\n",
      "Epoch 26/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5315 - acc: 0.7292 - val_loss: 0.5300 - val_acc: 0.7327\n",
      "Epoch 27/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5264 - acc: 0.7331 - val_loss: 0.5268 - val_acc: 0.7382\n",
      "Epoch 28/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5252 - acc: 0.7328 - val_loss: 0.5378 - val_acc: 0.7294\n",
      "Epoch 29/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5245 - acc: 0.7321 - val_loss: 0.5350 - val_acc: 0.7298\n",
      "Epoch 30/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5249 - acc: 0.7343 - val_loss: 0.5292 - val_acc: 0.7399\n",
      "Epoch 31/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5205 - acc: 0.7345 - val_loss: 0.5244 - val_acc: 0.7395\n",
      "Epoch 32/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5179 - acc: 0.7332 - val_loss: 0.5291 - val_acc: 0.7374\n",
      "Epoch 33/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5154 - acc: 0.7345 - val_loss: 0.5637 - val_acc: 0.7193\n",
      "Epoch 34/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5163 - acc: 0.7373 - val_loss: 0.5295 - val_acc: 0.7340\n",
      "Epoch 35/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5152 - acc: 0.7375 - val_loss: 0.5191 - val_acc: 0.7462\n",
      "Epoch 36/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5101 - acc: 0.7364 - val_loss: 0.5588 - val_acc: 0.7197\n",
      "Epoch 37/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5100 - acc: 0.7396 - val_loss: 0.5158 - val_acc: 0.7513\n",
      "Epoch 38/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5101 - acc: 0.7385 - val_loss: 0.5181 - val_acc: 0.7492\n",
      "Epoch 39/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5060 - acc: 0.7436 - val_loss: 0.5335 - val_acc: 0.7370\n",
      "Epoch 40/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5049 - acc: 0.7456 - val_loss: 0.5257 - val_acc: 0.7365\n",
      "Epoch 41/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.5024 - acc: 0.7453 - val_loss: 0.5229 - val_acc: 0.7471\n",
      "Epoch 42/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4994 - acc: 0.7458 - val_loss: 0.5291 - val_acc: 0.7370\n",
      "Epoch 43/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4972 - acc: 0.7458 - val_loss: 0.5055 - val_acc: 0.7555\n",
      "Epoch 44/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4938 - acc: 0.7498 - val_loss: 0.5198 - val_acc: 0.7370\n",
      "Epoch 45/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4926 - acc: 0.7504 - val_loss: 0.5359 - val_acc: 0.7214\n",
      "Epoch 46/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4964 - acc: 0.7454 - val_loss: 0.5045 - val_acc: 0.7462\n",
      "Epoch 47/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4881 - acc: 0.7532 - val_loss: 0.5025 - val_acc: 0.7572\n",
      "Epoch 48/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4893 - acc: 0.7523 - val_loss: 0.4984 - val_acc: 0.7534\n",
      "Epoch 49/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4892 - acc: 0.7498 - val_loss: 0.5028 - val_acc: 0.7546\n",
      "Epoch 50/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4874 - acc: 0.7532 - val_loss: 0.5333 - val_acc: 0.7311\n",
      "Epoch 51/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4824 - acc: 0.7551 - val_loss: 0.5253 - val_acc: 0.7319\n",
      "Epoch 52/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4794 - acc: 0.7615 - val_loss: 0.4992 - val_acc: 0.7555\n",
      "Epoch 53/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4794 - acc: 0.7584 - val_loss: 0.4993 - val_acc: 0.7538\n",
      "Epoch 54/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4738 - acc: 0.7593 - val_loss: 0.5014 - val_acc: 0.7525\n",
      "Epoch 55/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4738 - acc: 0.7584 - val_loss: 0.4999 - val_acc: 0.7542\n",
      "Epoch 56/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4736 - acc: 0.7622 - val_loss: 0.4918 - val_acc: 0.7546\n",
      "Epoch 57/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4672 - acc: 0.7639 - val_loss: 0.5137 - val_acc: 0.7492\n",
      "Epoch 58/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4694 - acc: 0.7624 - val_loss: 0.4944 - val_acc: 0.7647\n",
      "Epoch 59/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4635 - acc: 0.7689 - val_loss: 0.4973 - val_acc: 0.7466\n",
      "Epoch 60/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4649 - acc: 0.7706 - val_loss: 0.4964 - val_acc: 0.7555\n",
      "Epoch 61/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4627 - acc: 0.7670 - val_loss: 0.5105 - val_acc: 0.7508\n",
      "Epoch 62/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4613 - acc: 0.7690 - val_loss: 0.4902 - val_acc: 0.7588\n",
      "Epoch 63/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4601 - acc: 0.7719 - val_loss: 0.4998 - val_acc: 0.7475\n",
      "Epoch 64/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4598 - acc: 0.7707 - val_loss: 0.5000 - val_acc: 0.7567\n",
      "Epoch 65/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4534 - acc: 0.7749 - val_loss: 0.4962 - val_acc: 0.7572\n",
      "Epoch 66/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4543 - acc: 0.7753 - val_loss: 0.4910 - val_acc: 0.7559\n",
      "Epoch 67/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4488 - acc: 0.7778 - val_loss: 0.4931 - val_acc: 0.7555\n",
      "Epoch 68/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4593 - acc: 0.7694 - val_loss: 0.4908 - val_acc: 0.7580\n",
      "Epoch 69/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4472 - acc: 0.7790 - val_loss: 0.5009 - val_acc: 0.7635\n",
      "Epoch 70/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4475 - acc: 0.7748 - val_loss: 0.4833 - val_acc: 0.7664\n",
      "Epoch 71/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4462 - acc: 0.7772 - val_loss: 0.4830 - val_acc: 0.7618\n",
      "Epoch 72/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4447 - acc: 0.7794 - val_loss: 0.5050 - val_acc: 0.7534\n",
      "Epoch 73/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4425 - acc: 0.7795 - val_loss: 0.4903 - val_acc: 0.7656\n",
      "Epoch 74/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4420 - acc: 0.7815 - val_loss: 0.4936 - val_acc: 0.7508\n",
      "Epoch 75/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4393 - acc: 0.7857 - val_loss: 0.4787 - val_acc: 0.7719\n",
      "Epoch 76/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4349 - acc: 0.7891 - val_loss: 0.4849 - val_acc: 0.7660\n",
      "Epoch 77/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4356 - acc: 0.7885 - val_loss: 0.4898 - val_acc: 0.7736\n",
      "Epoch 78/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4374 - acc: 0.7860 - val_loss: 0.4856 - val_acc: 0.7757\n",
      "Epoch 79/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4290 - acc: 0.7909 - val_loss: 0.4850 - val_acc: 0.7639\n",
      "Epoch 80/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4307 - acc: 0.7905 - val_loss: 0.4669 - val_acc: 0.7761\n",
      "Epoch 81/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4247 - acc: 0.7890 - val_loss: 0.4831 - val_acc: 0.7719\n",
      "Epoch 82/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4254 - acc: 0.7913 - val_loss: 0.4648 - val_acc: 0.7774\n",
      "Epoch 83/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4222 - acc: 0.7900 - val_loss: 0.4625 - val_acc: 0.7828\n",
      "Epoch 84/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4203 - acc: 0.7994 - val_loss: 0.4724 - val_acc: 0.7774\n",
      "Epoch 85/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4158 - acc: 0.7999 - val_loss: 0.5083 - val_acc: 0.7673\n",
      "Epoch 86/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4174 - acc: 0.7979 - val_loss: 0.4686 - val_acc: 0.7782\n",
      "Epoch 87/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4148 - acc: 0.7986 - val_loss: 0.4793 - val_acc: 0.7715\n",
      "Epoch 88/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4140 - acc: 0.7997 - val_loss: 0.4783 - val_acc: 0.7622\n",
      "Epoch 89/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4118 - acc: 0.8002 - val_loss: 0.4590 - val_acc: 0.7854\n",
      "Epoch 90/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4089 - acc: 0.8006 - val_loss: 0.4659 - val_acc: 0.7786\n",
      "Epoch 91/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4120 - acc: 0.8015 - val_loss: 0.4684 - val_acc: 0.7799\n",
      "Epoch 92/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4083 - acc: 0.8052 - val_loss: 0.4517 - val_acc: 0.7883\n",
      "Epoch 93/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4070 - acc: 0.8046 - val_loss: 0.4715 - val_acc: 0.7896\n",
      "Epoch 94/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4118 - acc: 0.8005 - val_loss: 0.4663 - val_acc: 0.7866\n",
      "Epoch 95/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.4015 - acc: 0.8083 - val_loss: 0.4838 - val_acc: 0.7605\n",
      "Epoch 96/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.3992 - acc: 0.8058 - val_loss: 0.4480 - val_acc: 0.7896\n",
      "Epoch 97/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.3964 - acc: 0.8103 - val_loss: 0.4570 - val_acc: 0.7799\n",
      "Epoch 98/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.3957 - acc: 0.8096 - val_loss: 0.4679 - val_acc: 0.7904\n",
      "Epoch 99/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.3913 - acc: 0.8120 - val_loss: 0.4526 - val_acc: 0.7946\n",
      "Epoch 100/100\n",
      "11088/11088 [==============================] - 0s - loss: 0.3913 - acc: 0.8122 - val_loss: 0.4544 - val_acc: 0.7904\n"
     ]
    }
   ],
   "source": [
    "# deep CNN model\n",
    "\n",
    "model = Sequential()\n",
    "# 10*5*5\n",
    "model.add(Convolution2D(16, 3, 3,input_shape=(10, 5, 5),border_mode='valid'))\n",
    "model.add(ELU())\n",
    "# 8*3*16\n",
    "model.add(Convolution2D(32, 3, 3,border_mode='valid'))\n",
    "model.add(ELU())\n",
    "# 6*1*32\n",
    "model.add(Flatten())\n",
    "# 192\n",
    "model.add(Dense(64))\n",
    "model.add(ELU())\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "history_object = model.fit(samples_scaled, one_hot_labels, \\\n",
    "                           validation_data=(samples_scaled_validation, one_hot_labels_validation), \\\n",
    "                           nb_epoch=100, batch_size=64, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15840/15840 [==============================] - 1s - loss: 0.6122 - acc: 0.6653     \n",
      "Epoch 2/10\n",
      "15840/15840 [==============================] - 0s - loss: 0.5772 - acc: 0.6956     \n",
      "Epoch 3/10\n",
      "15840/15840 [==============================] - 0s - loss: 0.5679 - acc: 0.7033     \n",
      "Epoch 4/10\n",
      "15840/15840 [==============================] - 0s - loss: 0.5653 - acc: 0.7029     \n",
      "Epoch 5/10\n",
      "15840/15840 [==============================] - 0s - loss: 0.5603 - acc: 0.7091     \n",
      "Epoch 6/10\n",
      "15840/15840 [==============================] - 0s - loss: 0.5590 - acc: 0.7107     \n",
      "Epoch 7/10\n",
      "15840/15840 [==============================] - 0s - loss: 0.5551 - acc: 0.7119     \n",
      "Epoch 8/10\n",
      "15840/15840 [==============================] - 0s - loss: 0.5514 - acc: 0.7159     \n",
      "Epoch 9/10\n",
      "15840/15840 [==============================] - 0s - loss: 0.5489 - acc: 0.7145     \n",
      "Epoch 10/10\n",
      "15840/15840 [==============================] - 0s - loss: 0.5492 - acc: 0.7128     \n"
     ]
    }
   ],
   "source": [
    "# deep ANN model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(10, 5, 5)))\n",
    "# 192\n",
    "model.add(Dense(64))\n",
    "model.add(ELU())\n",
    "model.add(Dense(32))\n",
    "model.add(ELU())\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "history_object = model.fit(samples_scaled, one_hot_labels, nb_epoch=10, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.89720249,  0.10279749],\n",
       "       [ 0.9130066 ,  0.08699342],\n",
       "       [ 0.90403706,  0.09596292],\n",
       "       [ 0.93120247,  0.06879755],\n",
       "       [ 0.85641062,  0.14358936],\n",
       "       [ 0.88387764,  0.1161223 ],\n",
       "       [ 0.91124195,  0.08875811],\n",
       "       [ 0.95300812,  0.04699193],\n",
       "       [ 0.86156821,  0.13843176],\n",
       "       [ 0.89209193,  0.10790803]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(samples_scaled[0:10,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_for_shallow(samples_scaled):\n",
    "    num_features = samples_scaled.shape[3]\n",
    "    pave_condition1 = samples_scaled[:,:,:,num_features-2]\n",
    "    pave_condition1 = np.reshape(pave_condition1,(pave_condition1.shape[0],pave_condition1.shape[1]*pave_condition1.shape[2]))\n",
    "    pave_condition1 = np.sum(pave_condition1,axis = 1)\n",
    "    pave_condition1[pave_condition1>0]=1\n",
    "    pave_condition1 = np.reshape(pave_condition1,(pave_condition1.shape[0],1))\n",
    "    pave_condition2 = samples_scaled[:,:,:,num_features-1]\n",
    "    pave_condition2 = np.reshape(pave_condition2,(pave_condition2.shape[0],pave_condition2.shape[1]*pave_condition2.shape[2]))\n",
    "    pave_condition2 = np.sum(pave_condition2,axis = 1)\n",
    "    pave_condition2[pave_condition2>0]=1\n",
    "    pave_condition2 = np.reshape(pave_condition2,(pave_condition2.shape[0],1))\n",
    "    \n",
    "    traffic = samples_scaled[:,:,:,:num_features-2]\n",
    "    data = np.reshape(traffic,(traffic.shape[0],traffic.shape[1]*traffic.shape[2]*traffic.shape[3]))\n",
    "    data = np.concatenate((data,pave_condition1,pave_condition2),axis=1)\n",
    "    return data\n",
    "\n",
    "data_shallow = data_for_shallow(samples_scaled)\n",
    "data_shallow_validation = data_for_shallow(samples_scaled_validation)\n",
    "data_shallow_test = data_for_shallow(samples_scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15840, 152)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_shallow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.695977633478\n",
      "0.693181818182\n",
      "0.672138047138\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = SVC()\n",
    "clf.fit(data_shallow, labels)\n",
    "pred = clf.predict(data_shallow)\n",
    "acc = accuracy_score(pred, labels)\n",
    "pred_validation = clf.predict(data_shallow_validation)\n",
    "acc_validation = accuracy_score(pred_validation, labels_validation)\n",
    "pred_test = clf.predict(data_shallow_test)\n",
    "acc_test = accuracy_score(pred_test, labels_test)\n",
    "print(acc)\n",
    "print(acc_validation)\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97123015873\n",
      "0.765151515152\n",
      "0.741161616162\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(min_samples_split=10)\n",
    "clf.fit(data_shallow, labels)\n",
    "pred = clf.predict(data_shallow)\n",
    "acc = accuracy_score(pred, labels)\n",
    "pred_validation = clf.predict(data_shallow_validation)\n",
    "acc_validation = accuracy_score(pred_validation, labels_validation)\n",
    "pred_test = clf.predict(data_shallow_test)\n",
    "acc_test = accuracy_score(pred_test, labels_test)\n",
    "print(acc)\n",
    "print(acc_validation)\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:CarND]",
   "language": "python",
   "name": "conda-env-CarND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
